{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MDMAGl8hsh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f735f3-7dda-4d90-a98c-47f7f0908b11"
      },
      "source": [],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fse\n",
            "  Downloading fse-1.0.0.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.10/dist-packages (from fse) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from fse) (1.10.1)\n",
            "Requirement already satisfied: smart_open>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fse) (6.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from fse) (1.2.2)\n",
            "Requirement already satisfied: gensim>=4 in /usr/local/lib/python3.10/dist-packages (from fse) (4.3.1)\n",
            "Collecting wordfreq>=2.2.1 (from fse)\n",
            "  Downloading wordfreq-3.0.3-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub (from fse)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fse) (5.9.5)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->fse) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->fse) (3.1.0)\n",
            "Collecting ftfy>=6.1 (from wordfreq>=2.2.1->fse)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (3.3.0)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (1.0.5)\n",
            "Requirement already satisfied: regex>=2021.7.6 in /usr/local/lib/python3.10/dist-packages (from wordfreq>=2.2.1->fse) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->fse) (23.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1->wordfreq>=2.2.1->fse) (0.2.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->fse) (3.4)\n",
            "Building wheels for collected packages: fse\n",
            "  Building wheel for fse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fse: filename=fse-1.0.0-cp310-cp310-linux_x86_64.whl size=301842 sha256=d0a28966645b8e83cc5de3ceb4eb7fa7699f92405c48a5395cafee456ce91cd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8a/53/df807a30bb717ec61730522005512e2dd2656e3eca7e5f60f7\n",
            "Successfully built fse\n",
            "Installing collected packages: ftfy, wordfreq, huggingface-hub, fse\n",
            "Successfully installed fse-1.0.0 ftfy-6.1.1 huggingface-hub-0.15.1 wordfreq-3.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.29.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Cloning into 'rep'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 185 (delta 28), reused 161 (delta 18), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (185/185), 2.42 MiB | 25.30 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "/content/rep\n",
            "S08_question_answer_pairs.txt  S10_question_answer_pairs.txt  text_data_toc.csv\n",
            "S09_question_answer_pairs.txt  text_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pmhgxb6h96F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663b6249-c035-4646-ab89-672baeef7e90"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import re\n",
        "import fse\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from fse import SplitIndexedList\n",
        "from fse.models import uSIF\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import logging\n",
        "import re\n",
        "import unicodedata\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "<ipython-input-3-beeaa5b37d76>:25: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  pd.set_option('display.max_colwidth', -1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vlR9fAliBoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0b2400-08b7-426d-b8aa-f5052101e868"
      },
      "source": [
        "# Reading in the dataset\n",
        "df1 = pd.read_csv('/content/S08_question_answer_pairs.txt', sep='\\t')\n",
        "df2 = pd.read_csv('/content/S09_question_answer_pairs.txt', sep='\\t')\n",
        "df3 = pd.read_csv('/content/S10_question_answer_pairs.txt', sep='\\t', encoding = 'ISO-8859-1')\n",
        "frames = [df1, df2, df3]\n",
        "df = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-31f83d272daa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reading in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/S08_question_answer_pairs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/S09_question_answer_pairs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/S10_question_answer_pairs.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/S08_question_answer_pairs.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0nymiUmiGSl"
      },
      "source": [
        "def getArticleText(file):\n",
        "  fpath = './dataset/text_data/'+file+'.txt.clean'\n",
        "  try:\n",
        "    f = open(fpath, 'r')\n",
        "    text = f.read()\n",
        "  except UnicodeDecodeError:\n",
        "    f = open(fpath, 'r', encoding = 'ISO-8859-1')\n",
        "    text = f.read()\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQeE475GiWTH"
      },
      "source": [
        "df = df.dropna(subset=['ArticleFile'])\n",
        "df = df.dropna(subset=['Answer'])\n",
        "df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))\n",
        "df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\\n)+', '. ', x))\n",
        "df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwjS2fCjiWVz"
      },
      "source": [
        "# stop_words = set(stopwords.words('english'))\n",
        "def cleanQuestion(text):\n",
        "  text = str(text)\n",
        "  # wnl = nltk.stem.WordNetLemmatizer()\n",
        "  text = text.lower()\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  # words = [word for word in words if not word in stop_words]\n",
        "  return \" \".join([word for word in words])\n",
        "\n",
        "def cleanAnswer(text):\n",
        "  text = str(text)\n",
        "  # wnl = nltk.stem.WordNetLemmatizer()\n",
        "  text = text.lower()\n",
        "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
        "  # words = [word for word in words if not word in stop_words]\n",
        "  return \" \".join([word for word in words])\n",
        "\n",
        "def cleanText(text):\n",
        "  text = str(text)\n",
        "  # wnl = nltk.stem.WordNetLemmatizer()\n",
        "  text = text.lower()\n",
        "  words = re.sub(r'[^\\w\\s\\.\\?]', '', text).split()\n",
        "  # words = [word for word in words if not word in stop_words]\n",
        "  return \" \".join([word for word in words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYCWxxVdiWZV"
      },
      "source": [
        "df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))\n",
        "df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))\n",
        "df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKmGfZkOi4Sg"
      },
      "source": [
        "# Preparing data for training\n",
        "allQuestion = '. '.join(list(df['Question']))\n",
        "allAnswer = '. '.join(list(df['Answer']))\n",
        "allContext = '. '.join(list(df['ArticleText']))\n",
        "text = allQuestion + allAnswer + allContext\n",
        "s = SplitIndexedList(text.split('.'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzlxMm3ijoVm"
      },
      "source": [
        "## **Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ8ifZVXjJJa"
      },
      "source": [
        "dataset = []\n",
        "title = \"\"\n",
        "for i in range(0, len(df), 2):\n",
        "    this_title = df.iloc[i]['ArticleTitle']\n",
        "    if (this_title!=title):\n",
        "        title = this_title\n",
        "        text = df.iloc[i]['ArticleText']\n",
        "        splitted = text.split(sep='.')\n",
        "        for j in range(len(splitted)):\n",
        "            text = splitted[j]\n",
        "            if(text!=''):\n",
        "                words = text.split()\n",
        "                dataset.append(words)\n",
        "    dataset.append(df.iloc[i]['Question'].split())\n",
        "    dataset.append(df.iloc[i]['Answer'].split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HV0tiRanYCA"
      },
      "source": [
        "model_wv = gensim.models.Word2Vec(dataset, size=100, window=8, min_count=1, sg=0, workers=8) # I have 8 cpu cores\n",
        "# sg = {0, 1} – Training algorithm: 1 for skip-gram; otherwise CBOW"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4XP5-bWnZzE"
      },
      "source": [
        "model_wv.train(dataset, total_examples=len(dataset), compute_loss=True, epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0AGhAwinbaz"
      },
      "source": [
        "def get_embedding(sentence):\n",
        "  pos_sum = [0.0 for i in range(100)]\n",
        "  num = 0\n",
        "  words = sentence.split()\n",
        "  for i in words:\n",
        "    try:\n",
        "      embed = model_wv.wv[i]\n",
        "    except:\n",
        "      continue\n",
        "    else:\n",
        "      pos_sum += embed\n",
        "      num +=1\n",
        "  if(num==0):\n",
        "    return pos_sum\n",
        "  else:\n",
        "    pos_sum /= num\n",
        "    return pos_sum\n",
        "\n",
        "# get answer using euclidean distance\n",
        "def get_answer(question, answer_para):\n",
        "  question_embedding = get_embedding(rem_stop(question))\n",
        "  min_distance = math.inf\n",
        "  answer = 0\n",
        "  for i in range(len(answer_para)):\n",
        "    answer_embedding = get_embedding(rem_stop(answer_para[i]))\n",
        "    distance = np.linalg.norm(question_embedding-answer_embedding)\n",
        "    if (distance < min_distance):\n",
        "      answer = i\n",
        "      # print(answer)\n",
        "      min_distance = distance\n",
        "  return answer_para[answer]\n",
        "\n",
        "def rem_stop(sentence):\n",
        "    strr=''\n",
        "    my_string = sentence.split()\n",
        "    for i in range(len(my_string)):\n",
        "        if my_string[i] not in stopwords.words('english'):\n",
        "            strr = strr+' '+my_string[i]\n",
        "    return strr[1:]\n",
        "\n",
        "# get answer using cosine similarity\n",
        "def get_answer_cosine(question, answer_para):\n",
        "  question_embedding = get_embedding(rem_stop(question))\n",
        "  max_similarity = -math.inf\n",
        "  answer = 0\n",
        "  for i in range(len(answer_para)):\n",
        "    answer_embedding = get_embedding(rem_stop(answer_para[i]))\n",
        "    similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))\n",
        "    if (similarity > max_similarity):\n",
        "      answer = i\n",
        "      max_similarity = similarity\n",
        "  return answer_para[answer]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qWxvJ70ndwu"
      },
      "source": [
        "index = 296\n",
        "my_text = df.iloc[index]['ArticleText']\n",
        "temp_sentences = my_text.split(sep='.')\n",
        "sentences=[]\n",
        "for i in range(len(temp_sentences)):\n",
        "    if(temp_sentences[i]!=''):\n",
        "        sentences.append(temp_sentences[i])\n",
        "my_question = df.iloc[index]['Question']\n",
        "\n",
        "# Function to break up article text into individual sentences\n",
        "def contextToSents(my_text):\n",
        "  temp_sentences = my_text.split(sep='.')\n",
        "  sentences=[]\n",
        "  for i in range(len(temp_sentences)):\n",
        "      if(temp_sentences[i]!=''):\n",
        "          sentences.append(temp_sentences[i])\n",
        "  return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CjyVzT2nf-R"
      },
      "source": [
        "# Word2Vec outputs\n",
        "print(my_question) # Actual Question\n",
        "print(rem_stop(my_question)) # Answer without stopwords\n",
        "print(df.iloc[index]['Answer']) # Actual Answer\n",
        "print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance\n",
        "print(\"\\n\")\n",
        "print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7ipSnbUjs_M"
      },
      "source": [
        "## **SIF Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jap2--tvkB-N"
      },
      "source": [
        "from fse.models import uSIF\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")\n",
        "model_sif = uSIF(glove, workers=2, lang_freq=\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVnbzCO9kFXD"
      },
      "source": [
        "model_sif.train(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f39fFAmlkU2C"
      },
      "source": [
        "q = 'was abraham lincoln the sixteenth president of the united states'\n",
        "c1 = 'Abraham Lincoln (February 12, 1809 – April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination.'\n",
        "c2 = 'Scholars now rank Lincoln among the top three U.S. Presidents, with the majority of those surveyed placing him first.'\n",
        "c3 = 'Symbolic log cabin at Abraham Lincoln Birthplace National Historic Site.'\n",
        "\n",
        "tmpq = (q.split(), 0)\n",
        "tmpc1 = (c1.split(), 0)\n",
        "tmpc2 = (c2.split(), 0)\n",
        "tmpc3 = (c3.split(), 0)\n",
        "\n",
        "print(metrics.pairwise.cosine_similarity(model_sif.infer([tmpq]), model_sif.infer([tmpc1])))\n",
        "print(metrics.pairwise.cosine_similarity(model_sif.infer([tmpq]), model_sif.infer([tmpc2])))\n",
        "print(metrics.pairwise.cosine_similarity(model_sif.infer([tmpq]), model_sif.infer([tmpc3])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ2mTWZLkdDY"
      },
      "source": [
        "def getSim(q, x):\n",
        "  x = (str(x).split(), 0)\n",
        "  sim = metrics.pairwise.cosine_similarity(model_sif.infer([q]), model_sif.infer([x]))\n",
        "  return sim\n",
        "\n",
        "def getAnswer(question, context):\n",
        "  q = (str(question).split(), 0)\n",
        "  c = pd.DataFrame(str(context).split('.'))\n",
        "  c['sim'] = c[0].apply(lambda x: getSim(q, x))\n",
        "  max = c.sort_values(by='sim', ascending=False).iloc[:3]\n",
        "  return max\n",
        "\n",
        "def getBestAnswer(question, potentials):\n",
        "  q = (str(question).split(), 0)\n",
        "  c = pd.DataFrame(potentials)\n",
        "  c['sim'] = c[0].apply(lambda x: getSim(q, x))\n",
        "  max = c.sort_values(by='sim', ascending=False).iloc[:3]\n",
        "  return max[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIVXyzmQkfmU"
      },
      "source": [
        "qno = 250\n",
        "print('Question: ', df['Question'].iloc[qno])\n",
        "print('Top Solutions: \\n')\n",
        "print(getAnswer(df['Question'].iloc[qno], df['ArticleText'].iloc[qno]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hsnVAHFkjOD"
      },
      "source": [
        "qno = 67\n",
        "print('Question: ', df['Question'].iloc[qno])\n",
        "print('Top Solutions: \\n')\n",
        "sol = getAnswer(df['Question'].iloc[qno], df['ArticleText'].iloc[qno])\n",
        "print(sol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYydaVRikuzr"
      },
      "source": [
        "## **BERT with SIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU1YwGRXloRT"
      },
      "source": [
        "use_cuda = True\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "model_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6UsMts0lpoH"
      },
      "source": [
        "def getGraph(start_scores, end_scores, tokens):\n",
        "\n",
        "  # x-axis: unique tokens\n",
        "  # y-axis: start word score for each token\n",
        "\n",
        "  # Pull the scores from Tensors and convert to 1-D np arrays\n",
        "  start_scores = start_scores.detach().numpy().flatten()\n",
        "  end_scores = end_scores.detach().numpy().flatten()\n",
        "\n",
        "  # We add unique token index to each token\n",
        "  token_label = list()\n",
        "  for (i, t) in enumerate(tokens):\n",
        "      token_label.append('{:} - {:>2}'.format(t, i))\n",
        "\n",
        "  # Plot Graph\n",
        "  sns.set(style='darkgrid')\n",
        "  plt.rcParams[\"figure.figsize\"] = (24,8)\n",
        "  ax = sns.barplot(x=token_labels, y=s_scores, ci=None)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "  ax.grid(True)\n",
        "  plt.title('Start Word Scores for Each unique Token in Context')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCzyccb0lrYI"
      },
      "source": [
        "model_sif.train(SplitIndexedList(list(df['ArticleText'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHfMskFl5tG"
      },
      "source": [
        "def get_split(text1):\n",
        "\n",
        "  #Reference: https://medium.com/@armandj.olivares/using-bert-for-classifying-documents-with-long-texts-5c3e7b04573d\n",
        "\n",
        "  l_total = []\n",
        "  l_parcial = []\n",
        "  if len(text1.split())//150 >0:\n",
        "    n = len(text1.split())//150\n",
        "  else:\n",
        "    n = 1\n",
        "  for w in range(n):\n",
        "    if w == 0:\n",
        "      l_parcial = text1.split()[:250]\n",
        "      l_total.append(\" \".join(l_parcial))\n",
        "    else:\n",
        "      l_parcial = text1.split()[w*150:w*150 + 250]\n",
        "      l_total.append(\" \".join(l_parcial))\n",
        "  return l_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPjN_KK4mFU_"
      },
      "source": [
        "def getAnswerBert(question, context):\n",
        "\n",
        "  # print('Query Context has {} tokens.'.format(len(tokenizer.encode(context))))\n",
        "\n",
        "  context_list = get_split(context)\n",
        "\n",
        "  ans = []\n",
        "\n",
        "  for c in context_list:\n",
        "\n",
        "    encoding = tokenizer.encode_plus(text=question,text_pair=c)\n",
        "\n",
        "    inputs = encoding['input_ids']  #Token embeddings\n",
        "    token_type_id = encoding['token_type_ids']  #Segment embeddings\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs) #input tokens\n",
        "\n",
        "    output = model_bert(input_ids=torch.tensor([inputs]), token_type_ids=torch.tensor([token_type_id]))\n",
        "    start_index = torch.argmax(output.start_logits)\n",
        "    end_index = torch.argmax(output.end_logits)\n",
        "\n",
        "    answer = ' '.join(tokens[start_index:end_index+1])\n",
        "\n",
        "    ans.append(answer)\n",
        "  print('Question: ', question)\n",
        "\n",
        "  potentials = []\n",
        "  for i in ans:\n",
        "    if ('SEP' not in i) and ('CLS' not in i):\n",
        "      potentials.append(re.sub('(#)+', '', i))\n",
        "\n",
        "  answer = getBestAnswer(question, potentials)\n",
        "\n",
        "  # print('Potential Answers: \\n')\n",
        "  # print(answer.head())\n",
        "  return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0k8ZgBLmGeq"
      },
      "source": [
        "print(getAnswerBert(df['Question'].iloc[171], df['ArticleText'].iloc[171]))\n",
        "print('Actual Answer: ', df['Answer'].iloc[171])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUWSx6kjmPvj"
      },
      "source": [
        "print(getAnswerBert(df['Question'].iloc[255], df['ArticleText'].iloc[255]))\n",
        "print('Actual Answer: ', df['Answer'].iloc[255])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h9S2iOKmTYJ"
      },
      "source": [
        "## **Comparision**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tz2-zvVDfZI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1JQkmCymWAU"
      },
      "source": [
        "# Function to compare all 3 of our models\n",
        "\n",
        "def compare(indexes):\n",
        "  wv_sim = []\n",
        "  sif_sim = []\n",
        "  bert_sim = []\n",
        "  for i in indexes:\n",
        "    question = df.iloc[i]['Question']\n",
        "    context = df.iloc[i]['ArticleText']\n",
        "    answer = df.iloc[i]['Answer']\n",
        "\n",
        "    if ('yes' in answer.lower()) or ('no' in answer.lower()):\n",
        "      continue\n",
        "\n",
        "    ans_sif = model_sif.infer([((answer).split(), 0)])\n",
        "\n",
        "    w = metrics.pairwise.cosine_similarity(model_sif.infer([((get_answer_cosine(question, contextToSents(context))).split(), 0)]), ans_sif)\n",
        "    s = metrics.pairwise.cosine_similarity(model_sif.infer([((getAnswer(question, context).iloc[0][0]).split(), 0)]), ans_sif)\n",
        "\n",
        "    b_ans = getAnswerBert(question, context)\n",
        "    b = metrics.pairwise.cosine_similarity(model_sif.infer([((b_ans[b_ans.keys()[0]]).split(), 0)]), ans_sif)\n",
        "\n",
        "    wv_sim.append(w)\n",
        "    sif_sim.append(s)\n",
        "    bert_sim.append(b)\n",
        "\n",
        "  print('WV:', wv_sim)\n",
        "  print('SIF:', sif_sim)\n",
        "  print('BERT:', bert_sim)\n",
        "\n",
        "  return wv_sim, sif_sim, bert_sim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtapMjBG6LPw"
      },
      "source": [
        "# Function to compare all 3 of our models visually\n",
        "def visualise(i):\n",
        "\n",
        "  question = df.iloc[i]['Question']\n",
        "  context = df.iloc[i]['ArticleText']\n",
        "  answer = df.iloc[i]['Answer']\n",
        "\n",
        "  w_ans = get_answer_cosine(question, contextToSents(context))\n",
        "  s_ans = getAnswer(question, context).iloc[0][0]\n",
        "  b_ans = getAnswerBert(question, context)\n",
        "\n",
        "  print('Question: ', question)\n",
        "  print('Real: ', answer)\n",
        "  print('WV: ', w_ans)\n",
        "  print('SIF: ', s_ans)\n",
        "  print('BERT: ', b_ans[b_ans.keys()[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77vf0eCf9aPJ"
      },
      "source": [
        "visualise(130)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFypQM0n-MC2"
      },
      "source": [
        "visualise(234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF-TryFA-MO8"
      },
      "source": [
        "visualise(192)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB0XIj5dGRCT"
      },
      "source": [
        "visualise(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUY02HX8GRGF"
      },
      "source": [
        "visualise(666)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spCFOmMQGRKD"
      },
      "source": [
        "visualise(555)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECUvPNUy-MWb"
      },
      "source": [
        "visualise(1222)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYbQ3-eA-MZy"
      },
      "source": [
        "visualise(342)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ErBeUpt-Mc8"
      },
      "source": [
        "visualise(872)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_aKYbbS-MiD"
      },
      "source": [
        "visualise(1242)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttZepLYuBtjs"
      },
      "source": [
        "import random\n",
        "randomlist = random.sample(range(0, len(df)), 200)\n",
        "output = compare(list(randomlist))\n",
        "# print(output)\n",
        "# print(len(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHlnJkJxGo5M"
      },
      "source": [
        "# Final metrics (Average Cosine Similarity Over SIF Embeddings)\n",
        "print('Word2Vec Avg Similarity: ', sum(output[0])/len(output[0]))\n",
        "print('SIF Avg Similarity: ', sum(output[1])/len(output[1]))\n",
        "print('BERTwSIF Avg Similarity: ', sum(output[2])/len(output[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU84QqW_ACWI"
      },
      "source": [
        "# Sample user question picked up from the internet along with context\n",
        "getAnswerBert(('What do they teach at Ashoka?').lower(), ('Ashoka University is a pioneer in its focus on providing a liberal education at par with the best in the world. The aim at Ashoka is to help students become well-rounded individuals who can think critically about issues from multiple perspectives, communicate effectively and become leaders with a commitment to public service. An Ashoka education carries a strong emphasis on foundational knowledge, thorough academic research based on rigorous pedagogy, and hands-on experience with real-world challenging. The 2000-plus students on campus, drawn from 30 states and over 243 cities in India and 27 other countries, receive a world-class interdisciplinary education through undergraduate and post-graduate programmes led by internationally renowned faculty.').lower())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}